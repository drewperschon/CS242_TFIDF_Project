{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c2f11fa-dc6c-4a54-8241-485f43b21db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Drew\n",
      "[nltk_data]     Perschon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Drew\n",
      "[nltk_data]     Perschon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Drew\n",
      "[nltk_data]     Perschon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a3b31bb-1611-49b8-8e19-ce27f0bace41",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) #stopwords download (use)\n",
    "lemmatizer = WordNetLemmatizer() #lemmatizer download (use)\n",
    "stemmer = PorterStemmer() #stemmer download (maybe use)\n",
    "\n",
    "texts = [] #get docs\n",
    "keys = [] #get doc words\n",
    "\n",
    "for f in os.listdir(\".\"): #load from directory\n",
    "    if f.endswith(\".txt\"):\n",
    "        with open(f, \"r\", encoding=\"utf-8\",errors=\"ignore\") as file:\n",
    "            texts.append(file.read())\n",
    "            keys.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ff96dc0-35e4-4f51-9900-461fd70cb8d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'text' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(t)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;66;03m#conditions\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[1;32m---> 15\u001b[0m docs \u001b[38;5;241m=\u001b[39m [clean_text(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m     16\u001b[0m vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(word \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m doc))\n",
      "Cell \u001b[1;32mIn[44], line 2\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_text\u001b[39m(filepath):  \n\u001b[1;32m----> 2\u001b[0m     start \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** START\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#removing headers and footers\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     end \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*** END\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'text' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "def clean_text(filepath):  \n",
    "    start = text.find(\"*** START\") #removing headers and footers\n",
    "    end = text.find(\"*** END\")\n",
    "    if start != -1 and end != -1:\n",
    "        text = text[start:end]\n",
    "\n",
    "    text = text.lower() #converting to lowercase\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text) #replacing all punctuation and nonalphanumeric\n",
    "\n",
    "    tokens = text.split() #tokenize\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words if len(t)>2] #conditions\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "docs = [clean_text(t) for t in texts]\n",
    "vocab = sorted(set(word for doc in docs for word in doc)) #get all total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4dbf58f2-bf2f-4c34-94f4-17f1fe35ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_matrix(docs, keys, vocab):\n",
    "    matrix = pd.DataFrame(0, index=keys, columns=vocab, dtype=float)\n",
    "\n",
    "    for key, tokens in zip(keys, docs):\n",
    "        for token in tokens:\n",
    "            matrix.at[key, token] += 1\n",
    "        matrix.loc[key] = matrix.loc[key]/len(tokens) #normalize\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "630816ab-e855-4964-b56b-cf39b5e43e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tfidf(matrix, docs, vocab):\n",
    "    N = len(docs) #num of docs\n",
    "    df = (matrix>0).sum(axis=0) #amount of docs containing term\n",
    "    idf = np.log((1+N)/(1+df)) +1 #change when more files uploaded\n",
    "    idf = pd.Series(idf, index=vocab)\n",
    "\n",
    "    tfidf = matrix * idf\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "195aa8f8-5a95-488a-bb85-ecca16e1ad15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Similarity to BSA Handbook:\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'bsa_handbook.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'bsa_handbook.txt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m sim_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(similar, index\u001b[38;5;241m=\u001b[39mkeys, columns\u001b[38;5;241m=\u001b[39mkeys) \u001b[38;5;66;03m#comparing each book\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Similarity to BSA Handbook:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(sim_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbsa_handbook.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbsa_handbook.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)) \u001b[38;5;66;03m#final comparison with bsa_handbook.txt\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbsa_handbook.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'bsa_handbook.txt'"
     ]
    }
   ],
   "source": [
    "matrix = freq_matrix(docs, keys, vocab)\n",
    "tfidf = calc_tfidf(matrix, docs, vocab)\n",
    "\n",
    "similar = cosine_similarity(tfidf)\n",
    "sim_df = pd.DataFrame(similar, index=keys, columns=keys) #comparing each book\n",
    "\n",
    "print(\"\\n Similarity to BSA Handbook:\\n\")\n",
    "print(sim_df[\"bsa_handbook.txt\"].drop(\"bsa_handbook.txt\").sort_values(ascending=False)) #final comparison with bsa_handbook.txt\n",
    "\n",
    "for doc in keys:\n",
    "    if doc == \"bsa_handbook.txt\":\n",
    "        continue\n",
    "    diff = tfidf.loc[doc] - tfidf.loc[\"bsa_handbook.txt\"]\n",
    "    unique = diff.sort_values(ascending=False).head(10)\n",
    "    print(f\"\\nTop distinctive words in {doc} compared to BSA handbook:\")\n",
    "    print(unique)\n",
    "\n",
    "\n",
    "similarities = sim_df[\"bsa_handbook.txt\"].drop(\"bsa_handbook.txt\").sort_values()\n",
    "\n",
    "colors = [\"purple\"] * len(similarities) #regular color\n",
    "colors[list(similarities.index).index(similarities.idxmax())] = \"green\" #top color\n",
    "colors[list(similarities.index).index(similarities.idxmin())] = \"red\" #bottom color\n",
    "\n",
    "plot = similarities.plot(kind=\"barh\", figsize=[8,5], color=colors) #make a pretty bar plot\n",
    "plt.title(\"Similarity of Scouting Aspects Compared to 1911 BSA Handbook\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "\n",
    "plt.xlim(0, similarities.max() + 0.1) #expanding x axis\n",
    "\n",
    "for i, (val, name) in enumerate(zip(similarities, similarities.index)):\n",
    "    plt.text(val+0.01, i, f\"{val:.4f}\", va=\"center\")\n",
    "\n",
    "most_patch = mpatches.Patch(color=\"green\", label=\"Most similar to BSA handbook\") #legend\n",
    "least_patch = mpatches.Patch(color=\"red\", label=\"Least similar to BSA handbook\")\n",
    "other_patch = mpatches.Patch(color=\"purple\", label=\"Other\")\n",
    "plt.legend(handles=[most_patch, least_patch, other_patch], loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b8bf602-f4c2-4e89-aeb0-ee248d548b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Drew Perschon\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f71c8d-efc7-4838-aa41-8a47c3363803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
